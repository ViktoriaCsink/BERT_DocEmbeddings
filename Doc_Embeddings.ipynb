{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fatal-clone",
   "metadata": {},
   "source": [
    "## This script calculates the cosine similarity between a reference text and texts in Class0 and texts in Class1. <br> Class1 texts are related to the reference text, Class0 texts are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "raising-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base = '/Users/Viktoria/Desktop/NLP_DocEmbeddings'\n",
    "os.chdir(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advisory-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import docx2txt\n",
    "\n",
    "from read_file import read_file\n",
    "from clean_text import lemmatize\n",
    "from clean_text import clean_text\n",
    "\n",
    "pd.options.display.max_rows = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "narrative-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "passive-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appointed-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/jbarlow83/OCRmyPDF.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-cameroon",
   "metadata": {},
   "source": [
    "## Step 1. Access the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "molecular-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant documents:  22 \n",
      "Irrelevant documents:  20\n"
     ]
    }
   ],
   "source": [
    "#Get the texts\n",
    "\n",
    "class1 = [doc for doc in os.listdir(os.path.join(base, 'Raw_data/Class1'))]\n",
    "class0 = [doc for doc in os.listdir(os.path.join(base, 'Raw_data/Class0'))]\n",
    "\n",
    "print('Relevant documents: ', len(class1), '\\nIrrelevant documents: ', len(class0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "neural-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Document', 'Class', 'Directory'])\n",
    "\n",
    "row=0\n",
    "for doc in class1:\n",
    "    df.loc[row, 'Document'] = doc\n",
    "    df.loc[row, 'Class'] = 1\n",
    "    df.loc[row, 'Directory'] = os.path.join(base, 'Raw_data/Class1')\n",
    "    row=row+1\n",
    "    \n",
    "for doc in class0:\n",
    "    df.loc[row, 'Document'] = doc\n",
    "    df.loc[row, 'Class'] = 0\n",
    "    df.loc[row, 'Directory'] = os.path.join(base, 'Raw_data/Class0')\n",
    "    row=row+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "athletic-spider",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 38/42 [02:30<00:11,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psr_mr18_1_3_pass-through_analysis_consultation_february_2019.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [02:42<00:00,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162.3767900466919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#loop through them all at once. \n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df['Text'] = df.progress_apply(lambda x: read_file(x['Document'], x['Directory']), axis=1)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-antibody",
   "metadata": {},
   "source": [
    "## Step 2. Text cleaning and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alternative-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A set of words we want to screen out as corpus-specific stop words, i.e. 'article', 'paragraph', etc\n",
    "\n",
    "os.chdir(os.path.join(base, 'Documents'))\n",
    "\n",
    "useless = docx2txt.process(\"useless_words.docx\")\n",
    "useless = re.findall(r'\\w+', useless)\n",
    "useless = lemmatize(useless)\n",
    "useless = [u.lower() for u in useless]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gorgeous-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All non-English words will be disposed of. Keep important non-English words on this list.\n",
    "\n",
    "informative = docx2txt.process(\"informative_words.docx\")\n",
    "informative = re.findall(r'\\w+', informative)\n",
    "informative = [i.lower() for i in informative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-miami",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 12/42 [00:17<00:59,  2.00s/it]"
     ]
    }
   ],
   "source": [
    "#loop through them all at once.\n",
    "\n",
    "df['Cleaned_Text'] = df['Text'].progress_apply(clean_text, args=(useless,informative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-glossary",
   "metadata": {},
   "source": [
    "### Create a dataframe where each row is a document. Add cosine similarity score between document & set of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keywords\n",
    "os.chdir(main)\n",
    "\n",
    "with open('Stablecoin_keyphrases.txt', 'r+') as f:\n",
    "    keyphrases = f.readlines()  \n",
    "    \n",
    "keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "keyphrases = [re.sub('\\n', '', k) for k in keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many words do we have?\n",
    "keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Join_Text'] = [' '.join(text) for text in df.Cleaned_Text]\n",
    "\n",
    "df.Join_Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get word embeddings for the keywords and the documents\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "for r,v in df.iterrows():\n",
    "    \n",
    "    text = df.loc[r, 'Join_Text']\n",
    "    doc_embedding = model.encode([text])\n",
    "    simlist = []\n",
    "     \n",
    "    for k in keyphrases:\n",
    "                        \n",
    "        sim = cosine_similarity(doc_embedding, model.encode([k]))\n",
    "        num = float(sim)\n",
    "        simlist.append(num)\n",
    "        \n",
    "    df.loc[r, 'Keyphrase_Similarity'] = mean(simlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean similarity in each class\n",
    "\n",
    "sims = df.groupby('Class')['Similarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion: The differences are miniscule..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-volunteer",
   "metadata": {},
   "source": [
    "### Compare to reference text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(main)\n",
    "\n",
    "#Add any txt file with reference text\n",
    "with open('JMLSG Section 22.txt', 'r+') as f:\n",
    "    reftext = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final text cleaning. This creates a single, cleaned string out of each item in the collection of texts\n",
    "\n",
    "def prepare_extract(text):\n",
    "    \n",
    "    res = ' '.join([' '.join(clean_text(t)) for t in text])\n",
    "        \n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "reftext = prepare_extract(reftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "reftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "for r,v in df.iterrows():\n",
    "    df.loc[r, 'Join_Text'] = ' '.join(df.loc[r, 'Cleaned_Text'])\n",
    "\n",
    "candidate_embeddings = model.encode([reftext])\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for r,v in df.iterrows():\n",
    "    text = df.loc[r, 'Join_Text']\n",
    "    doc_embedding = model.encode([text])\n",
    "    all_embeddings.extend(doc_embedding)\n",
    "    df.loc[r, 'Reftext_Similarity'] = float(cosine_similarity(doc_embedding, candidate_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = df.groupby('Class')['Reftext_Similarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-image",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interior-sandwich",
   "metadata": {},
   "source": [
    "### Check the statistical probability of the occurrence of keywords in the non-stablecoin texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "stablecoin = ' '.join(df.Join_Text[0:10])\n",
    "words1 = stablecoin.split()\n",
    "other = ' '.join(df.Join_Text[10:20])\n",
    "words2 = other.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-payroll",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "words1.count('stablecoin')/len(words1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2.count('stablecoin')/len(words2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-analyst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "billion-lloyd",
   "metadata": {},
   "source": [
    "### Calculate the number of times the key phrases ocurred in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevants = df\n",
    "cleaned_data = []\n",
    "cleaned_data.extend(' '.join(d for d in relevants['Cleaned_Text'][index]) for index, row in relevants.iterrows())\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "cv.fit(cleaned_data)\n",
    "\n",
    "cleaned_data_transformed = cv.transform(cleaned_data)\n",
    "\n",
    "data = pd.DataFrame(cleaned_data_transformed.toarray(),\n",
    "                 columns=cv.get_feature_names())\n",
    "\n",
    "data = data.drop([col for col in data.columns if len(col.split())==1] , axis='columns')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keywords\n",
    "os.chdir(main)\n",
    "\n",
    "with open('Stablecoin_keyphrases.txt', 'r+') as f:\n",
    "    keyphrases = f.readlines()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = [re.sub('\\n', '', k) for k in keyphrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all the irrelevant collocations\n",
    "\n",
    "data = data.drop(columns=[col for col in data.columns if col not in keyphrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Phrases'] = data.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_readable(text):\n",
    "    \n",
    "    if type(text) == bytes:\n",
    "        text = text.decode(\"utf-8\") \n",
    "    text = re.findall(r'[A-Za-z0-9/./,]*', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = [' '.join(human_readable(t)) for t in df.Join_Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Phrases_Normalised'] = df.apply(lambda row: row.Phrases/len(row.Cleaned_Text)*100, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Text', 'Class', 'Cleaned_Text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'Human_Text': 'Text'}, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Document', 'Text', 'Phrases', 'Phrases_Normalised']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(stable_coin)\n",
    "\n",
    "df.to_csv('Stablecoin.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-conjunction",
   "metadata": {},
   "source": [
    "### Visualise document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = df.Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = pd.DataFrame(data=all_embeddings, index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for dimensionality reduction\n",
    "\n",
    "#Computing the correlation matrix\n",
    "X_corr=embeds.corr()\n",
    "\n",
    "#Computing eigen values and eigen vectors\n",
    "values,vectors=np.linalg.eig(X_corr)\n",
    "\n",
    "#Sorting the eigen vectors coresponding to eigen values in descending order\n",
    "args = (-values).argsort()\n",
    "values = vectors[args]\n",
    "vectors = vectors[:, args]\n",
    "\n",
    "#Taking first 2 components which explain maximum variance for projecting\n",
    "new_vectors=vectors[:,:2]\n",
    "\n",
    "#Projecting it onto new dimesion with 2 axis\n",
    "neww_X=np.dot(embeds,new_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "neww_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "#plt.scatter(neww_X[:,0],neww_X[:,1],linewidths=10,color='blue')\n",
    "plt.xlabel(\"PC1\",size=10)\n",
    "plt.ylabel(\"PC2\",size=10)\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "plt.title(\"Word Embedding Space\",size=20)\n",
    "#plt.tight_layout()\n",
    "\n",
    "vocab=list(embeds.index)\n",
    "for i, word in enumerate(vocab):\n",
    "    if word in stablecoin:\n",
    "        #plt.annotate(word,xy=(neww_X[i,0],neww_X[i,1]), color='red')\n",
    "        plt.scatter(neww_X[i,0],neww_X[i,1],linewidths=10,color='red')\n",
    "    elif word in aml:\n",
    "        #plt.annotate(word,xy=(neww_X[i,0],neww_X[i,1]), color='blue')\n",
    "        plt.scatter(neww_X[i,0],neww_X[i,1],linewidths=10,color='blue')\n",
    "    elif word in crypto:\n",
    "        #plt.annotate(word,xy=(neww_X[i,0],neww_X[i,1]), color='orange')\n",
    "        plt.scatter(neww_X[i,0],neww_X[i,1],linewidths=10,color='orange')\n",
    "    elif word in payments:\n",
    "        #plt.annotate(word,xy=(neww_X[i,0],neww_X[i,1]), color='black')\n",
    "        plt.scatter(neww_X[i,0],neww_X[i,1],linewidths=10,color='black')\n",
    "        \n",
    "os.chdir(main)\n",
    "plt.savefig('Stablecoin embedding space2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-hampshire",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
